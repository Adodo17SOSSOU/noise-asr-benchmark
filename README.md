# ğŸ”ª Noise-Robust ASR Benchmark

This project benchmarks the performance of two Automatic Speech Recognition (ASR) models â€” **Whisper** and **Wav2Vec2** â€” under clean and noisy audio conditions using a subset of the [Common Voice](https://commonvoice.mozilla.org/) dataset.

---

## ğŸ”€ Project Pipeline

1. **Data Preparation**

   * Extract and clean Common Voice samples
   * Add Gaussian noise to generate a noisy subset

2. **Inference**

   * Run Whisper and Wav2Vec2 on both clean and noisy samples

3. **Evaluation**

   * Compute Word Error Rate (WER) and Character Error Rate (CER)

4. **Reporting**

   * Save results to CSV and generate bar chart comparisons

---

## ğŸ“ Project Structure

```
noise_asr_benchmark/
â”œâ”€â”€ data/                  # Contains clean and noisy datasets
â”œâ”€â”€ logs/                 # Slurm log outputs
â”œâ”€â”€ models/               # Can be used for saving model checkpoints
â”œâ”€â”€ results/              # WER/CER metrics, plots and leaderboard
â”‚   â”œâ”€â”€ leaderboard.csv
â”‚   â”œâ”€â”€ leaderboard_cer.png
â”‚   â””â”€â”€ leaderboard_wer.png
â”œâ”€â”€ scripts/              # All processing and evaluation scripts
â”‚   â”œâ”€â”€ add_noise.py
â”‚   â”œâ”€â”€ compute_metrics.py
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ generate_leaderboard.py
â”‚   â”œâ”€â”€ visualize_results.py
â”‚   â”œâ”€â”€ whisper_infer.py
â”‚   â”œâ”€â”€ wav2vec_infer.py
â”‚   â”œâ”€â”€ run_whisper.sh
â”‚   â”œâ”€â”€ run_wav2vec.sh
â”‚   â””â”€â”€ run_metrics.sh
â”œâ”€â”€ utils/                # Helper functions (e.g., metrics)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # You're reading it!
```

---

## ğŸ“Š Leaderboard

| Model    | Data  | WER   | CER   |
| -------- | ----- | ----- | ----- |
| Whisper  | clean | 0.121 | 0.051 |
| Whisper  | noisy | 0.253 | 0.143 |
| Wav2Vec2 | clean | 0.259 | 0.079 |
| Wav2Vec2 | noisy | 0.414 | 0.198 |

Plots:

* `results/leaderboard_wer.png`
* `results/leaderboard_cer.png`

---

## ğŸš€ How to Run

```bash
# Activate your environment
conda activate asr_env

# 1. Prepare data
python scripts/download_data.py
python scripts/add_noise.py

# 2. Inference
bash scripts/run_whisper.sh
bash scripts/run_wav2vec.sh

# 3. Evaluate
bash scripts/run_metrics.sh

# 4. Visualize
python scripts/generate_leaderboard.py
python scripts/visualize_results.py
```

---

## ğŸ§ Key Insights

* Whisper is significantly more robust to noise than Wav2Vec2.
* Wav2Vec2â€™s WER increases drastically under noisy conditions.
* Clean modular design allows easy extension (e.g., new ASR models, noise types).

---

## ğŸ›  Dependencies

Install via:

```bash
pip install -r requirements.txt
```

---

## ğŸªª License

MIT License Â© 2025 A. Sossou

